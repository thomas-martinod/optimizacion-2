{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Find the absolute minimum of the function\n",
    "\n",
    "\\begin{equation}\n",
    "    f(x) = f(x_1, x_2) = x_1 e^{-{x_1}^2 -{x_2}^2}\n",
    "\\end{equation}\n",
    "\n",
    "in the domain $x \\in \\mathbb{R}^2$ (unconstrained problem).\n",
    "\n",
    "The initial point is $x^0 = (-0.6, -0.3)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations performed: 4\n",
      "The point that minimizes the function f is [-42.96138907704416, -24.101290136910723], such that f(x) = -0.0\n",
      "   Iteration        x1       x2      f(x)  Gradient Norm           λ       β\n",
      "0          0 -0.400000 -0.01000 -0.340823       0.579440   -1.498286  1.0000\n",
      "1          1 -0.092869 -0.01819 -0.092041       0.973995   -0.500680  0.2500\n",
      "2          2  0.519800 -0.07208  0.394673       0.353583    5.808796  0.0625\n",
      "3          3  1.613004  0.46434  0.096396       0.266685 -120.373839  0.1250\n"
     ]
    }
   ],
   "source": [
    "## Levenberg-Marquardt algorithm (2nd order)\n",
    "\n",
    "import sympy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sympy.utilities.lambdify import lambdify\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "def LevenbergMarquardt(f, x0, ε, J):\n",
    "    # Define symbolic variables for optimization\n",
    "    x1, x2 = sp.symbols('x1 x2')\n",
    "\n",
    "    # Calculate the gradient (first derivative) of the objective function\n",
    "    grad_f = sp.Matrix([sp.diff(f, x1), sp.diff(f, x2)])\n",
    "\n",
    "    # Calculate the Hessian matrix (second derivative) of the objective function\n",
    "    hessian_f = sp.Matrix([[sp.diff(grad_f[i], x1), sp.diff(grad_f[i], x2)] for i in range(2)])\n",
    "\n",
    "    # Create lambdified functions for numerical computation\n",
    "    f_lambda = lambdify((x1, x2), f, 'numpy')\n",
    "    grad_lambda = lambdify((x1, x2), grad_f, 'numpy')\n",
    "    hessian_lambda = lambdify((x1, x2), hessian_f, 'numpy')\n",
    "\n",
    "    # Initialize x with the initial point\n",
    "    x = np.array(x0, dtype=float)\n",
    "    num_iterations = 0\n",
    "\n",
    "    # The starting β is one\n",
    "    β = 1\n",
    "\n",
    "    # Create an empty list to store optimization data\n",
    "    optimization_data = []\n",
    "\n",
    "    λ = None  # Initialize λ\n",
    "\n",
    "    while num_iterations <= J:\n",
    "        g = np.array(grad_lambda(x[0], x[1]), dtype=float)\n",
    "        H = np.array(hessian_lambda(x[0], x[1]), dtype=float)\n",
    "        s = np.reshape(-np.linalg.inv((H + np.array(β*np.eye(2), dtype=float))) @ g, x.shape)\n",
    "        grad_norm = np.linalg.norm(g)\n",
    "        fx = f_lambda(x[0], x[1])\n",
    "\n",
    "        if grad_norm <= ε or num_iterations == J:\n",
    "            break\n",
    "\n",
    "        # Define a function for g(λ) that takes λ as an argument and returns f(x + λs)\n",
    "        g_lambda = lambda lam: f_lambda(x[0] - lam * s[0], x[1] - lam * s[1])\n",
    "\n",
    "        # Use minimize_scalar to find the optimal λ\n",
    "        result = minimize_scalar(g_lambda)\n",
    "        λ = float(result.x)\n",
    "\n",
    "        optimization_data.append({'Iteration': num_iterations, 'x1': x[0], 'x2': x[1], 'f(x)': fx, 'Gradient Norm': grad_norm, 'λ': λ, 'β': β})\n",
    "\n",
    "        prev = f_lambda(x[0], x[1])\n",
    "        x_next = x + λ*s\n",
    "        next = f_lambda(x_next[0], x_next[1])\n",
    "\n",
    "        if next < prev:\n",
    "            # Only update x if the new estimate improves the objective function\n",
    "            β = β*2\n",
    "        else:\n",
    "            β = β/4\n",
    "\n",
    "        x = x_next\n",
    "\n",
    "\n",
    "        num_iterations += 1\n",
    "\n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    optimization_data_df = pd.DataFrame(optimization_data)\n",
    "\n",
    "    print(\"Iterations performed:\", num_iterations)\n",
    "    print(\"The point that minimizes the function f is [\" + str(x[0]) + ', ' + str(x[1]) + '], such that f(x) = ' + str(fx))\n",
    "\n",
    "    return fx, optimization_data_df\n",
    "\n",
    "# Define your objective function correctly using 'x1_val' and 'x2_val'\n",
    "x1, x2 = sp.symbols('x1 x2')\n",
    "def f(x1_val, x2_val):\n",
    "    return x1_val * sp.exp(-x1_val**2 - x2_val**2)\n",
    "\n",
    "tolerance = 1e-15\n",
    "initial_point = [-0.4, -0.01]\n",
    "\n",
    "# Call the Newton's method with λ and β optimization\n",
    "result, optimization_data = LevenbergMarquardt(f(x1, x2), initial_point, tolerance, 100)\n",
    "\n",
    "# Now the 'optimization_data' DataFrame includes the 0th iteration and f(x) value.\n",
    "print(optimization_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
