{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2.\n",
    "\n",
    "To find the minimum of the function $f(x)$ given by,\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = (x - 2)^2 + (y - 2)^2\n",
    "\\end{equation}\n",
    "\n",
    "Subject to:\n",
    "\n",
    "\\begin{align*}\n",
    "x + y & \\geq 4 \\\\\n",
    "2x - y & \\geq 0\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, using the logarithmic penalty method, we obtain the problem:\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{x,\\mu} f(x) + \\mu \\phi(g(x)) = \\min_{x,\\mu} \\, (x - 2)^2 + (y - 2)^2 - \\mu(\\log(x + y - 4) + \\log(2x - y))\n",
    "\\end{equation}\n",
    "\n",
    "In this problem, we aim to minimize the function $f(x)$ while satisfying the given constraints by introducing a penalty term controlled by the parameter $f(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "    Iteration         x         y          f(x)  Gradient Norm\n",
      "0           0  3.000000  2.000000  1.000000e+00   2.000000e+00\n",
      "1           1  2.000000  2.000000  2.154995e-09   6.464992e-01\n",
      "2           2  2.000000  2.000000  2.085680e-09   3.232496e-01\n",
      "3           3  2.000000  2.000000  2.016365e-09   1.616248e-01\n",
      "4           4  2.000000  2.000000  1.947051e-09   8.081240e-02\n",
      "5           5  2.000000  2.000000  1.877736e-09   4.040620e-02\n",
      "6           6  2.000000  2.000000  1.808421e-09   2.020309e-02\n",
      "7           7  2.000000  2.000000  1.739107e-09   1.010154e-02\n",
      "8           8  2.000000  2.000000  1.669792e-09   5.050748e-03\n",
      "9           9  2.000000  2.000000  1.600480e-09   2.525334e-03\n",
      "10         10  2.000000  2.000000  1.531173e-09   1.262588e-03\n",
      "11         11  2.000000  2.000000  1.461889e-09   6.311357e-04\n",
      "12         12  2.000000  2.000000  1.392700e-09   3.152514e-04\n",
      "13         13  2.000000  2.000000  1.323884e-09   1.569951e-04\n",
      "14         14  2.000001  2.000001  1.256538e-09   7.725384e-05\n",
      "15         15  2.000002  2.000002  1.194656e-09   3.627230e-05\n",
      "16         16  2.000003  2.000003  1.149510e-09   1.430195e-05\n",
      "17         17  2.000004  2.000004  1.133181e-09   3.226826e-06\n",
      "18         18  2.000005  2.000005  1.131982e-09   1.828806e-07\n",
      "19         19  2.000005  2.000005  1.131978e-09   5.912204e-10\n",
      "20         20  2.000005  2.000005  1.131978e-09   6.032073e-15\n",
      "21         21  2.000005  2.000005  1.131978e-09   5.146827e-16\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sympy.utilities.lambdify import lambdify\n",
    "\n",
    "# Define the Newton method for optimization\n",
    "def Newton(f, x0, ε, J, mu):\n",
    "    # Define symbolic variables x and y\n",
    "    x, y = sp.symbols('x y')\n",
    "\n",
    "    # Calculate the gradient of the objective function\n",
    "    grad_f = sp.Matrix([sp.diff(f, x), sp.diff(f, y)])\n",
    "\n",
    "    # Calculate the Hessian matrix (second derivative) of the objective function\n",
    "    Hess_f = sp.Matrix([[sp.diff(grad_f[i], x), sp.diff(grad_f[i], y)] for i in range(2)])\n",
    "\n",
    "    # Create lambda functions for numerical evaluation from symbolic expressions\n",
    "    f_lambda = lambdify((x, y), f, 'numpy')  # Objective function\n",
    "    grad_lambda = lambdify((x, y), grad_f, 'numpy')  # Gradient\n",
    "    hessian_lambda = lambdify((x, y), Hess_f, 'numpy')  # Hessian\n",
    "\n",
    "    # Initialize the parameter vector x with the initial point x0\n",
    "    x = np.array(x0, dtype=float)\n",
    "    num_iterations = 0  # Iteration counter\n",
    "\n",
    "    # List to store optimization data at each iteration\n",
    "    optimization_data = []\n",
    "\n",
    "    # Start of the optimization loop\n",
    "    while num_iterations <= J:\n",
    "        # Calculate the gradient and Hessian at the current point\n",
    "        g = np.array(grad_lambda(x[0], x[1]), dtype=float)\n",
    "        H = np.array(hessian_lambda(x[0], x[1]), dtype=float)\n",
    "\n",
    "        grad_norm = np.linalg.norm(g)\n",
    "        fx = f_lambda(x[0], x[1])\n",
    "\n",
    "        # Append data to the list as a dictionary, separating x into x and y\n",
    "        optimization_data.append({'Iteration': num_iterations, 'x': x[0], 'y': x[1], 'f(x)': fx, 'Gradient Norm': grad_norm})\n",
    "\n",
    "        if grad_norm <= ε or num_iterations == J:\n",
    "            break\n",
    "\n",
    "        # Update the parameter vector x using the Newton method with the penalized objective function\n",
    "        x = x - np.reshape(np.dot(np.linalg.inv(H), g), x.shape)\n",
    "        num_iterations += 1\n",
    "\n",
    "    # Create a DataFrame to store optimization data\n",
    "    optimization_data_df = pd.DataFrame(optimization_data)\n",
    "\n",
    "    return fx, optimization_data_df\n",
    "\n",
    "# Define symbolic variables x, y, and mu\n",
    "x, y, μ = sp.symbols('x y mu')\n",
    "\n",
    "# Define the objective function f(x, y)\n",
    "f = (x - 2)**2 + (y - 2)**2 - μ*(sp.log(x + y - 4) + sp.log(2*x - y))\n",
    "\n",
    "# Set tolerance ε, the max number of iterations J, and the initial point x0\n",
    "ε = 1e-15\n",
    "J = 100\n",
    "x0 = [3, 2]\n",
    "\n",
    "# Set initial large mu value\n",
    "mu_value = 10**(-10)\n",
    "results = []\n",
    "opts = []\n",
    "\n",
    "# Call the Newton method for optimization\n",
    "result, optimization_data = Newton(f.subs(μ, mu_value), x0, ε, J, mu_value)\n",
    "\n",
    "# Print the DataFrame with all the info\n",
    "print('\\n')\n",
    "print(optimization_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimum of $f(x)$ found at $x^* = (2.000005, 2.000005) \\rightarrow (2,2)$ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
