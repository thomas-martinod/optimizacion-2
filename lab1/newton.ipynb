{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Find the absolute minimum of the function\n",
    "\n",
    "\\begin{equation}\n",
    "    f(x) = f(x_1, x_2) = x_1 e^{-{x_1}^2 -{x_2}^2}\n",
    "\\end{equation}\n",
    "\n",
    "in the domain $x \\in \\mathbb{R}^2$ (unconstrained problem).\n",
    "\n",
    "The initial point is $x^0 = (-0.6, -0.3)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations performed: 5\n",
      "The point that minimizes the function f is x* = [-0.7071067811865476, 0.0], such that f(x*) = -0.42888194248035333\n",
      "\n",
      "\n",
      "   Iteration        x1            x2      f(x)       |∇f(x)|\n",
      "0          0 -0.600000 -3.000000e-01 -0.382577  2.908032e-01\n",
      "1          1 -0.726126  8.738739e-02 -0.425314  8.090296e-02\n",
      "2          2 -0.706530 -1.485944e-03 -0.428881  1.613678e-03\n",
      "3          3 -0.707107  8.538448e-09 -0.428882  3.980727e-07\n",
      "4          4 -0.707107 -1.837995e-21 -0.428882  6.528111e-14\n",
      "5          5 -0.707107  0.000000e+00 -0.428882  1.110223e-16\n"
     ]
    }
   ],
   "source": [
    "## Newton's method (2nd order)\n",
    "\n",
    "import sympy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sympy.utilities.lambdify import lambdify\n",
    "\n",
    "# Define the Newton method for optimization\n",
    "def Newton(f, x0, ε, J):\n",
    "    # Define symbolic variables x1 and x2\n",
    "    x1, x2 = sp.symbols('x1 x2')\n",
    "\n",
    "    # Calculate the gradient of the objective function\n",
    "    grad_f = sp.Matrix([sp.diff(f, x1), sp.diff(f, x2)])\n",
    "\n",
    "    # Calculate the Hessian matrix (second derivative) of the objective function\n",
    "    Hess_f = sp.Matrix([[sp.diff(grad_f[i], x1), sp.diff(grad_f[i], x2)] for i in range(2)])\n",
    "\n",
    "    # Create lambda functions for numerical evaluation from symbolic expressions\n",
    "    f_lambda = lambdify((x1, x2), f, 'numpy')  # Objective function\n",
    "    grad_lambda = lambdify((x1, x2), grad_f, 'numpy')  # Gradient\n",
    "    hessian_lambda = lambdify((x1, x2), Hess_f, 'numpy')  # Hessian\n",
    "\n",
    "    # Initialize the parameter vector x with the initial point x0\n",
    "    x = np.array(x0, dtype=float)\n",
    "    num_iterations = 0  # Iteration counter\n",
    "\n",
    "    # List to store optimization data at each iteration\n",
    "    optimization_data = []\n",
    "\n",
    "    # Start of the optimization loop\n",
    "    while num_iterations <= J:\n",
    "        # Calculate the gradient and Hessian at the current point\n",
    "        g = np.array(grad_lambda(x[0], x[1]), dtype=float)\n",
    "        H = np.array(hessian_lambda(x[0], x[1]), dtype=float)\n",
    "\n",
    "        grad_norm = np.linalg.norm(g)\n",
    "        fx = f_lambda(x[0], x[1])\n",
    "\n",
    "        # Append data to the list as a dictionary, separating x into x1 and x2\n",
    "        optimization_data.append({'Iteration': num_iterations + 0, 'x1': x[0], 'x2': x[1], 'f(x)': fx, '|∇f(x)|': grad_norm})\n",
    "\n",
    "        if grad_norm <= ε or num_iterations == J:\n",
    "            break\n",
    "\n",
    "        # Update the parameter vector x using the Newton method\n",
    "        x = x - np.reshape(np.dot(np.linalg.inv(H), g), x.shape)\n",
    "        num_iterations += 1\n",
    "\n",
    "    # Create a DataFrame to store optimization data\n",
    "    optimization_data_df = pd.DataFrame(optimization_data)\n",
    "\n",
    "    # Print the number of iterations and the final result\n",
    "    print(\"Iterations performed:\", num_iterations)\n",
    "    print(\"The point that minimizes the function f is x* = [\" + str(x[0]) + ', ' + str(x[1]) + '], such that f(x*) = ' + str(fx))\n",
    "\n",
    "    return fx, optimization_data_df\n",
    "\n",
    "# Define symbolic variables x1 and x2\n",
    "x1, x2 = sp.symbols('x1 x2')\n",
    "\n",
    "# Define the objective function f(x1, x2)\n",
    "def f(x1_val, x2_val):\n",
    "    return x1_val * sp.exp(-x1_val**2 - x2_val**2)\n",
    "\n",
    "# Set tolerance ε, the max number of iterations J and the initial point x0\n",
    "ε = 1e-15\n",
    "J = 100\n",
    "x0 = [-0.6, -0.3]\n",
    "\n",
    "# Call the Newton method for optimization\n",
    "result, optimization_data = Newton(f(x1, x2), x0, ε, J)\n",
    "\n",
    "# Print the DataFrame with all tne info\n",
    "print('\\n')\n",
    "print(optimization_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal point is found at $x^* = (-0.7071, 0) \\rightarrow (-1/\\sqrt 2, 0)$ with the minimum value $f(x^*) = -0.4289 \\rightarrow -1/\\sqrt {2e}$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
