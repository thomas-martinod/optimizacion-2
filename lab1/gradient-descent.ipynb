{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Find the absolute minimum of the function\n",
    "\n",
    "\\begin{equation}\n",
    "    f(x) = f(x_1, x_2) = x_1 e^{-{x_1}^2 -{x_2}^2}\n",
    "\\end{equation}\n",
    "\n",
    "in the domain $x \\in \\mathbb{R}^2$ (unconstrained problem).\n",
    "\n",
    "The initial point is $x^0 = (-0.6, -0.3)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations performed: 71\n",
      "The point that minimizes the function f is x* = [-0.7071067811880666, 2.7211058675555406e-15], such that f(x*) = -0.42888194248035344\n",
      "\n",
      "\n",
      "    Iteration        x1            x2      f(x)       |∇f(x)|             λ\n",
      "0           0 -0.600000 -3.000000e-01 -0.382577  2.908032e-01  9.266659e-01\n",
      "1           1 -0.765443 -8.728743e-02 -0.422814  1.202278e-01  7.456717e-01\n",
      "2           2 -0.694677 -3.224738e-02 -0.428303  3.499490e-02  8.455856e-01\n",
      "3           3 -0.712845 -8.889514e-03 -0.428820  1.241814e-02  7.200949e-01\n",
      "4           4 -0.705786 -3.399510e-03 -0.428875  3.694085e-03  8.464618e-01\n",
      "..        ...       ...           ...       ...           ...           ...\n",
      "67         67 -0.707107  4.134883e-14 -0.428882  1.106592e-10  1.095448e+00\n",
      "68         68 -0.707107  2.495982e-15 -0.428882  9.729950e-11 -1.396333e+00\n",
      "69         69 -0.707107  5.485481e-15 -0.428882  3.303754e-10  5.875089e-01\n",
      "70         70 -0.707107  2.721106e-15 -0.428882  2.606028e-12  1.947564e-11\n",
      "71         71 -0.707107  2.721106e-15 -0.428882  2.606028e-12  1.947564e-11\n",
      "\n",
      "[72 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "## gradient-descent method (1st order)\n",
    "\n",
    "import sympy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sympy.utilities.lambdify import lambdify\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "# Define the gradient descent optimization function\n",
    "def GradientDescent(f, x0, ε, J):\n",
    "    x1, x2 = sp.symbols('x1 x2')\n",
    "\n",
    "    # Calculate the gradient and Hessian matrix\n",
    "    grad_f = sp.Matrix([sp.diff(f, x1), sp.diff(f, x2)])\n",
    "\n",
    "    # Create lambda functions for the objective function and gradient\n",
    "    f_lambda = lambdify((x1, x2), f, 'numpy')\n",
    "    grad_lambda = lambdify((x1, x2), grad_f, 'numpy')\n",
    "\n",
    "    # Initialize variables\n",
    "    x = np.array(x0, dtype=float)\n",
    "    num_iterations = 0\n",
    "    prev_grad = 0\n",
    "\n",
    "    optimization_data = []\n",
    "\n",
    "    λ = None  # Initialize λ\n",
    "\n",
    "    # Main gradient descent loop\n",
    "    while num_iterations <= J:\n",
    "        # Calculate the gradient at the current point\n",
    "        g = np.array(grad_lambda(x[0], x[1]), dtype=float)\n",
    "        grad_norm = np.linalg.norm(g)\n",
    "        fx = f_lambda(x[0], x[1])\n",
    "\n",
    "\n",
    "        if abs(grad_norm - prev_grad) <= ε or num_iterations == J:\n",
    "            break\n",
    "\n",
    "        prev_grad = grad_norm\n",
    "\n",
    "        # Define a function for g(λ) that takes λ as an argument and returns f(x + λs)\n",
    "        # λ minimices this function (line search)\n",
    "        g_lambda = lambda lam: f_lambda(x[0] - lam * g[0], x[1] - lam * g[1])\n",
    "\n",
    "        # Use minimize_scalar to find the optimal λ\n",
    "        result = minimize_scalar(g_lambda)\n",
    "        λ = float(result.x)\n",
    "\n",
    "        # Append data to the list as a dictionary, separating x into x1 and x2\n",
    "        optimization_data.append({'Iteration': num_iterations, 'x1': x[0], 'x2': x[1], 'f(x)': fx, '|∇f(x)|': grad_norm, 'λ': λ})\n",
    "        # Update x using the optimal λ and the gradient\n",
    "        x = x - np.reshape(λ * g, x.shape)\n",
    "        num_iterations += 1\n",
    "\n",
    "    # Create a DataFrame to store optimization data and add last iteration\n",
    "    optimization_data.append({'Iteration': num_iterations, 'x1': x[0], 'x2': x[1], 'f(x)': fx, '|∇f(x)|': grad_norm, 'λ': λ})\n",
    "    optimization_data_df = pd.DataFrame(optimization_data)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Iterations performed:\", num_iterations)\n",
    "    print(\"The point that minimizes the function f is x* = [\" + str(x[0]) + ', ' + str(x[1]) + '], such that f(x*) = ' + str(fx))\n",
    "\n",
    "    return fx, optimization_data_df\n",
    "\n",
    "# Define the symbols and objective function\n",
    "x1, x2 = sp.symbols('x1 x2')\n",
    "def f(x1_val, x2_val):\n",
    "    return x1_val * sp.exp(-x1_val**2 - x2_val**2)\n",
    "\n",
    "# Set tolerance, initial point, and search direction\n",
    "ε = 1e-15\n",
    "J = 100\n",
    "x0 = [-0.6, -0.3]\n",
    "\n",
    "# Call the gradient descent function\n",
    "result, optimization_data = GradientDescent(f(x1, x2), x0, ε, J)\n",
    "\n",
    "# Print the optimization data DataFrame\n",
    "print('\\n')\n",
    "print(optimization_data)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
